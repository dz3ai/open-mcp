{
  "type": "object",
  "properties": {
    "model": {
      "type": "string",
      "description": "The model to use for image generation.",
      "example": "fluently-xl"
    },
    "prompt": {
      "type": "string",
      "minLength": 1,
      "maxLength": 1500,
      "description": "The description for the image. Character limit is model specific and is listed in the promptCharacterLimit setting in the model list endpoint.",
      "example": "A beautiful sunset over a mountain range"
    },
    "negative_prompt": {
      "type": "string",
      "maxLength": 1500,
      "description": "A description of what should not be in the image. Character limit is model specific and is listed in the promptCharacterLimit constraint in the model list endpoint.",
      "example": "Clouds, Rain, Snow"
    },
    "style_preset": {
      "type": "string",
      "description": "An image style to apply to the image. Visit https://docs.venice.ai/apiv1imagegenerate for more details.",
      "example": "3D Model"
    },
    "height": {
      "type": "integer",
      "minimum": 0,
      "exclusiveMinimum": true,
      "maximum": 1280,
      "default": 1024,
      "description": "Height of the generated image. Each model has a specific height and width divisor listed in the widthHeightDivisor constraint in the model list endpoint.",
      "example": 1024
    },
    "width": {
      "type": "integer",
      "minimum": 0,
      "exclusiveMinimum": true,
      "maximum": 1280,
      "default": 1024,
      "description": "Width of the generated image. Each model has a specific height and width divisor listed in the widthHeightDivisor constraint in the model list endpoint.",
      "example": 1024
    },
    "steps": {
      "type": "integer",
      "minimum": 0,
      "exclusiveMinimum": true,
      "maximum": 50,
      "default": 20,
      "description": "Number of inference steps. The following models have reduced max steps from the global max: flux-dev: 30 max steps, flux-dev-uncensored: 30 max steps, stable-diffusion-3.5: 30 max steps, stable-diffusion-3.5-rev2: 30 max steps. These constraints are exposed in the model list endpoint for each model.",
      "example": 20
    },
    "cfg_scale": {
      "type": "number",
      "minimum": 0,
      "exclusiveMinimum": true,
      "maximum": 20,
      "description": "CFG scale parameter. Higher values lead to more adherence to the prompt.",
      "example": 7.5
    },
    "seed": {
      "type": "integer",
      "minimum": -999999999,
      "maximum": 999999999,
      "description": "Random seed for generation. If not provided, a random seed will be used.",
      "example": 123456789
    },
    "lora_strength": {
      "type": "integer",
      "minimum": 0,
      "maximum": 100,
      "description": "Lora strength for the model. Only applies if the model uses additional Loras.",
      "example": 50
    },
    "safe_mode": {
      "type": "boolean",
      "description": "Whether to use safe mode. If enabled, this will blur images that are classified as having adult content.",
      "example": false
    },
    "return_binary": {
      "type": "boolean",
      "default": false,
      "description": "Whether to return binary image data instead of base64.",
      "example": false
    },
    "hide_watermark": {
      "type": "boolean",
      "default": false,
      "description": "Whether to hide the Venice watermark. Venice may ignore this parameter for certain generated content.",
      "example": false
    },
    "format": {
      "type": "string",
      "enum": [
        "webp",
        "png"
      ],
      "default": "png",
      "description": "The image format to return. WebP are smaller and optimized for web use. PNG are higher quality but larger in file size. NOTE: This currently defaults to PNG but will change in the future to WebP. If you wish to receive PNGs, ensure your API call specifies the format.",
      "example": "webp"
    },
    "embed_exif_metadata": {
      "type": "boolean",
      "default": false,
      "description": "Embed prompt generation information into the image's EXIF metadata.",
      "example": false
    },
    "inpaint": {
      "type": "object",
      "description": "<llm-instruction>This part of the input schema is truncated. If you want to pass the property `inpaint` to the tool, first call the tool `expandSchema` with \"/properties/inpaint\" in the list of pointers. This will return the expanded input schema which you can then use in the tool call. You may have to call `expandSchema` multiple times if the schema is nested.</llm-instruction>",
      "additionalProperties": true
    },
    "Accept-Encoding": {
      "description": "Supported compression encodings (gzip, br). Only applied when return_binary is false.",
      "type": "string",
      "example": "gzip, br"
    }
  },
  "required": [
    "model",
    "prompt"
  ]
}