{
  "type": "object",
  "properties": {
    "messages": {
      "description": "A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).",
      "type": "array",
      "minItems": 1,
      "items": {
        "x-oaiExpandable": true,
        "anyOf": [
          {
            "type": "object",
            "title": "System message",
            "properties": {
              "content": {
                "description": "The contents of the system message.",
                "type": "string"
              },
              "role": {
                "type": "string",
                "enum": [
                  "system"
                ],
                "description": "The role of the messages author, in this case `system`."
              },
              "name": {
                "type": "string",
                "description": "An optional name for the participant. Provides the model information to differentiate between participants of the same role."
              }
            },
            "required": [
              "content",
              "role"
            ]
          }
        ]
      }
    },
    "model": {
      "description": "ID of the model to use. See the [model endpoint compatibility](https://platform.openai.com/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API.",
      "example": "gpt-4-turbo",
      "anyOf": [
        {
          "type": "string"
        },
        {
          "type": "string",
          "enum": [
            "gpt-4o",
            "gpt-4o-2024-05-13",
            "gpt-4-turbo",
            "gpt-4-turbo-2024-04-09",
            "gpt-4-0125-preview",
            "gpt-4-turbo-preview",
            "gpt-4-1106-preview",
            "gpt-4-vision-preview",
            "gpt-4",
            "gpt-4-0314",
            "gpt-4-0613",
            "gpt-4-32k",
            "gpt-4-32k-0314",
            "gpt-4-32k-0613",
            "gpt-3.5-turbo",
            "gpt-3.5-turbo-16k",
            "gpt-3.5-turbo-0301",
            "gpt-3.5-turbo-0613",
            "gpt-3.5-turbo-1106",
            "gpt-3.5-turbo-0125",
            "gpt-3.5-turbo-16k-0613"
          ]
        }
      ],
      "x-oaiTypeLabel": "string"
    },
    "frequency_penalty": {
      "type": "number",
      "default": 0,
      "minimum": -2,
      "maximum": 2,
      "nullable": true,
      "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n\n[See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation/parameter-details)\n"
    },
    "logit_bias": {
      "type": "object",
      "description": "<llm-instruction>This part of the input schema is truncated. If you want to pass the property `logit_bias` to the tool, first call the tool `expandSchema` with \"/properties/logit_bias\" in the list of pointers. This will return the expanded input schema which you can then use in the tool call. You may have to call `expandSchema` multiple times if the schema is nested.</llm-instruction>\n<property-description>Modify the likelihood of specified tokens appearing in the completion.\n\nAccepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n</property-description>",
      "additionalProperties": true
    },
    "logprobs": {
      "description": "Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.",
      "type": "boolean",
      "default": false,
      "nullable": true
    },
    "top_logprobs": {
      "description": "An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.",
      "type": "integer",
      "minimum": 0,
      "maximum": 20,
      "nullable": true
    },
    "max_tokens": {
      "description": "The maximum number of [tokens](https://platform.openai.com/tokenizer?view=bpe) that can be generated in the chat completion.\n\nThe total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.\n",
      "type": "integer",
      "nullable": true
    },
    "n": {
      "type": "integer",
      "minimum": 1,
      "maximum": 128,
      "default": 1,
      "example": 1,
      "nullable": true,
      "description": "How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs."
    },
    "presence_penalty": {
      "type": "number",
      "default": 0,
      "minimum": -2,
      "maximum": 2,
      "nullable": true,
      "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n\n[See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation/parameter-details)\n"
    },
    "response_format": {
      "type": "object",
      "description": "<llm-instruction>This part of the input schema is truncated. If you want to pass the property `response_format` to the tool, first call the tool `expandSchema` with \"/properties/response_format\" in the list of pointers. This will return the expanded input schema which you can then use in the tool call. You may have to call `expandSchema` multiple times if the schema is nested.</llm-instruction>\n<property-description>An object specifying the format that the model must output.\n\nSetting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }`enables Structured Outputs which ensures the model will match your\nsupplied JSON schema. Works across all the providers that support this functionality. [OpenAI & Azure OpenAI](/integrations/llms/openai/structured-outputs), [Gemini & Vertex AI](/integrations/llms/vertex-ai/controlled-generations).\n\nSetting to `{ \"type\": \"json_object\" }` enables the older JSON mode, which ensures the message the model generates is valid JSON.\n\nUsing `json_schema` is preferred for models that support it.\n</property-description>",
      "additionalProperties": true
    },
    "seed": {
      "type": "integer",
      "minimum": -9223372036854776000,
      "maximum": 9223372036854776000,
      "nullable": true,
      "description": "This feature is in Beta.\nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.\nDeterminism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.\n",
      "x-code-samples": {
        "beta": true
      }
    },
    "stop": {
      "description": "Up to 4 sequences where the API will stop generating further tokens.\n",
      "default": null,
      "anyOf": [
        {
          "type": "string",
          "nullable": true
        }
      ]
    },
    "stream": {
      "description": "If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-UShttps://platform.openai.com/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).\n",
      "type": "boolean",
      "nullable": true,
      "default": false
    },
    "stream_options": {
      "type": "object",
      "description": "<llm-instruction>This part of the input schema is truncated. If you want to pass the property `stream_options` to the tool, first call the tool `expandSchema` with \"/properties/stream_options\" in the list of pointers. This will return the expanded input schema which you can then use in the tool call. You may have to call `expandSchema` multiple times if the schema is nested.</llm-instruction>\n<property-description>Options for streaming response. Only set this when you set `stream: true`.\n</property-description>",
      "additionalProperties": true
    },
    "thinking": {
      "type": "object",
      "description": "<llm-instruction>This part of the input schema is truncated. If you want to pass the property `thinking` to the tool, first call the tool `expandSchema` with \"/properties/thinking\" in the list of pointers. This will return the expanded input schema which you can then use in the tool call. You may have to call `expandSchema` multiple times if the schema is nested.</llm-instruction>\n<property-description>View the thinking/reasoning tokens as part of your response. Thinking models produce a long internal chain of thought before generating a response. Supported only for specific Claude models on Anthropic, Google Vertex AI, and AWS Bedrock.  Requires setting `strict_openai_compliance = false` in your API call.\n</property-description>",
      "additionalProperties": true
    },
    "temperature": {
      "type": "number",
      "minimum": 0,
      "maximum": 2,
      "default": 1,
      "example": 1,
      "nullable": true,
      "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n\nWe generally recommend altering this or `top_p` but not both.\n"
    },
    "top_p": {
      "type": "number",
      "minimum": 0,
      "maximum": 1,
      "default": 1,
      "example": 1,
      "nullable": true,
      "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n\nWe generally recommend altering this or `temperature` but not both.\n"
    },
    "tools": {
      "type": "array",
      "description": "A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.\n",
      "items": {
        "type": "object",
        "properties": {
          "type": {
            "type": "string",
            "enum": [
              "function"
            ],
            "description": "The type of the tool. Currently, only `function` is supported."
          },
          "function": {
            "type": "object",
            "properties": {
              "description": {
                "type": "string",
                "description": "A description of what the function does, used by the model to choose when and how to call the function."
              },
              "name": {
                "type": "string",
                "description": "The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."
              },
              "parameters": {
                "type": "object",
                "description": "The parameters the functions accepts, described as a JSON Schema object. See the [guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format. \n\nOmitting `parameters` defines a function with an empty parameter list.",
                "additionalProperties": true
              },
              "strict": {
                "type": "boolean",
                "nullable": true,
                "default": false,
                "description": "Whether to enable strict schema adherence when generating the function call. If set to true, the model will follow the exact schema defined in the `parameters` field. Only a subset of JSON Schema is supported when `strict` is `true`. Learn more about Structured Outputs in the [function calling guide](docs/guides/function-calling)."
              }
            },
            "required": [
              "name"
            ]
          }
        },
        "required": [
          "type",
          "function"
        ]
      }
    },
    "tool_choice": {
      "description": "Controls which (if any) tool is called by the model.\n`none` means the model will not call any tool and instead generates a message.\n`auto` means the model can pick between generating a message or calling one or more tools.\n`required` means the model must call one or more tools.\nSpecifying a particular tool via `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to call that tool.\n\n`none` is the default when no tools are present. `auto` is the default if tools are present.\n",
      "x-oaiExpandable": true,
      "anyOf": [
        {
          "type": "string",
          "description": "`none` means the model will not call any tool and instead generates a message. `auto` means the model can pick between generating a message or calling one or more tools. `required` means the model must call one or more tools.\n",
          "enum": [
            "none",
            "auto",
            "required"
          ]
        }
      ]
    },
    "parallel_tool_calls": {
      "description": "Whether to enable [parallel function calling](https://platform.openai.com/docs/guides/function-calling/parallel-function-calling) during tool use.",
      "type": "boolean",
      "default": true
    },
    "user": {
      "type": "string",
      "example": "user-1234",
      "description": "A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).\n"
    },
    "x-portkey-trace-id": {
      "description": "An ID you can pass to refer to one or more requests later on. If not provided, Portkey generates a trace ID automatically for each request. [Docs](https://portkey.ai/docs/product/observability/traces)",
      "type": "string"
    },
    "x-portkey-span-id": {
      "description": "An ID you can pass to refer to a span under a trace.",
      "type": "string"
    },
    "x-portkey-parent-span-id": {
      "description": "Link a child span to a parent span",
      "type": "string"
    },
    "x-portkey-span-name": {
      "description": "Name for the Span ID",
      "type": "string"
    },
    "x-portkey-metadata": {
      "type": "object",
      "description": "<llm-instruction>This part of the input schema is truncated. If you want to pass the property `x-portkey-metadata` to the tool, first call the tool `expandSchema` with \"/properties/x-portkey-metadata\" in the list of pointers. This will return the expanded input schema which you can then use in the tool call. You may have to call `expandSchema` multiple times if the schema is nested.</llm-instruction>\n<property-description>Pass any arbitrary metadata along with your request</property-description>",
      "additionalProperties": true
    },
    "x-portkey-cache-namespace": {
      "description": "Partition your Portkey cache store based on custom strings, ignoring metadata and other headers",
      "type": "string"
    },
    "x-portkey-cache-force-refresh": {
      "description": "Forces a cache refresh for your request by making a new API call and storing the updated value",
      "type": "boolean"
    }
  },
  "required": [
    "messages",
    "model"
  ]
}